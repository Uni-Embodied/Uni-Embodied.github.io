<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Uni-Embodied: Towards Unified Evaluation for Embodied Planning, Perception, and Execution">
  <meta property="og:title" content="Uni-Embodied"/>
  <meta property="og:description" content="A benchmark for unified evaluation of planning, perception, and execution in embodied intelligence"/>
  <meta property="og:url" content="https://github.com/CRH380-CR400/uni-embodied"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="Uni-Embodied">
  <meta name="twitter:description" content="Benchmarking VLMs for unified embodied intelligence">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Embodied AI, VLM, Benchmark, Planning, Perception, Execution">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Uni-Embodied: Towards Unified Evaluation for Embodied Planning, Perception, and Execution</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img 
            src="static/images/favicon.ico"
            class="center"
            width="180"
          />
          <h1 class="title is-1 publication-title">Uni-Embodied: Towards Unified Evaluation for Embodied Planning, Perception, and Execution</h1>
         <div class="is-size-5 publication-authors" style="margin-top: 1rem;">
  <span class="author-block">
    <a class="author-link">Lingfeng Zhang</a><sup>1,*</sup>,
    <a>Yingbo Tang</a><sup>2,*</sup>,
    <a>Xinyu Zheng</a><sup>3</sup>,
    <a>Qiang Zhang</a><sup>4,6</sup>,
    <a>Yu Liu</a><sup>5</sup>,
    <a>Renjing Xu</a><sup>6</sup>,
    <a href="https://haoxiaoshuai.github.io/homepage/">Xiaoshuai Hao</a><sup>7,†</sup>
  </span>
  <br>
  <span class="author-block">
    <sup>1</sup>Shenzhen International Graduate School, Tsinghua University<br>
    <sup>2</sup>Institute of Automation, Chinese Academy of Sciences<br>
    <sup>3</sup>National Maglev Transportation Engineering Research and Development Center, Tongji University<br>
    <sup>4</sup>X-Humanoid<br>
    <sup>5</sup>Hefei University of Technology<br>
    <sup>6</sup>The Hong Kong University of Science and Technology (Guangzhou)<br>
    <sup>7</sup>Beijing Academy of Artificial Intelligence (BAAI)
  </span>
  <br>
  <span class="author-block">
    <sup>*</sup>Co-first Authors &nbsp;&nbsp;
    <sup>†</sup>Corresponding Author
  </span>
</div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="static/pdfs/ACM_MM_Uniembodied.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/linglingxiansen/Uni-Embodied-Eval" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
                </a>
              </span>
<!--               <span class="link-block">
                <a href="https://arxiv.org/abs/XXXX.XXXXX" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
                </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/llxs/Uni-Embodied"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
          <img 
              src="static/images/fig1.png"
              class="pipeline image"
              alt="pipeline image"
              style="margin-bottom: 20px;"
          />
      <h2 class="subtitle has-text-centered">
        This work introduces <strong>Uni-Embodied</strong> Benchmark, the first comprehensive benchmark designed to evaluate VLMs across the three key dimensions of embodied intelligence: planning, perception, and execution.
      </h2>
    </div>
  </div>
</section>


<!-- Abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Embodied intelligence is a core challenge in the pursuit of artificial general intelligence (AGI), requiring the seamless integration of <strong>planning, perception, and execution</strong> to enable agents to perform physical tasks effectively. While recent vision-language models (VLMs) have shown strong performance in isolated capabilities, their ability to jointly exhibit all three embodied skills remains unclear, impeding the development of unified embodied systems.
          </p>
          <p>
            In this paper, we propose <strong>Uni-Embodied</strong>, the first comprehensive benchmark designed to evaluate VLMs across the three key dimensions of embodied intelligence: planning, perception, and execution. Our benchmark includes nine diverse tasks—ranging from complex and simple embodied planning to trajectory summarization, map understanding, affordance recognition, spatial pointing, manipulation analysis, and execution in both navigation and manipulation contexts.
          </p>
          <p>
            Extensive experiments on leading open-source and closed-source VLMs demonstrate that current models struggle to achieve balanced performance across all three dimensions. Notably, we observe that enhancing planning and perception often compromises execution, while focusing on execution significantly degrades planning and perception capabilities—revealing fundamental limitations in existing approaches.
          </p>
          <p>
            We further explore strategies such as <strong>chain-of-thought prompting</strong> and <strong>hybrid training</strong> to selectively improve specific embodied capabilities. These findings offer valuable insights for the development of more robust and unified embodied intelligence systems, critical for advancing real-world robotic applications.
          </p>
        </div>
      </div>
    </div>
    

  </div>
</section>


<!-- Method Overview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Benchmark Definition</h2>
            <img src="static/images/fig2.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Overview of the Uni-Embodied Benchmark. 
                Our benchmark includes nine diverse tasks—ranging from complex and simple embodied planning to trajectory summarization, map understanding, affordance recognition, spatial pointing, manipulation analysis, and execution in both navigation and manipulation contexts.
              </p>
            </div>
          </div>
        </div>

        
        
      </div>
    </div>
  </div>
</section>

<!-- Method Overview -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Experimental Results</h2>
            <br>
            <h2 class="subtitle is-4">Planning Tasks</h2>
            <img src="static/images/leidatu.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Performance comparison of VLMs on embodied planning tasks including complex-task (navigation-manipulation combined) and simple-tasks (desktop manipulation only).
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Perception Tasks</h2>
            <img src="static/images/tab1.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Performance Comparison of VLMs on Embodied Perception Tasks.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Perception Tasks</h2>
            <img src="static/images/tab2.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Performance comparison of models on navigation and manipulation execution benchmark.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Ablation Study</h2>
            <img src="static/images/tab3.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Ablation experiments on chain-of-thought enhancement and hybrid training.
              </p>
            </div>
          </div>
        </div>
      </div>
      


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Dataset Samples</h2>
            <h2 class="subtitle is-4">Planning</h2>
            <img src="static/images/planning.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Complex planning task samples.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Navigation Trajectory Summarization</h2>
            <img src="static/images/navi_traject.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Navigation trajectory summarization samples.
              </p>
            </div>
          </div>
        </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Navigation Map Understanding</h2>
            <img src="static/images/map.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Navigation map understanding samples.
              </p>
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Manipulation Affordance Prediction</h2>
            <img src="static/images/affordance.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Manipulation affordance prediction samples.
              </p>
            </div>
          </div>
        </div>
      </div>
      

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="subtitle is-4">Manipulation Trajectory Analysis</h2>
            <img src="static/images/mani_traj.png" class="center-image blend-img-background">
            <div class="level-set has-text-justified">
              <p class="has-text-justified">
                Manipulation trajectory analysis samples.
              </p>
            </div>
          </div>
        </div>
      </div>
      
      <hr>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title is-3">Supplementary Materials</h2>
            <!-- <p class="has-text-justified">
              Here we introduce potential applications, reproducibility and licening and access.
            </p> -->
            <h2 class="subtitle is-4">Potential Applications</h2>
              <p class="has-text-justified">
                Our <strong>Uni-Embodied</strong> dataset and benchmark and its findings are important for advancing practical robotics applications across multiple domains. In the field of home robotics, a unified evaluation framework can guide the development of home robots that can seamlessly integrate planning ("prepare breakfast"), perception ("recognize kitchen items and spatial layout"), and execution ("navigate to appliances and manipulate utensils"). In the field of warehouse automation, robots must coordinate complex multi-step tasks such as inventory management, which requires sophisticated planning to optimize routes, strong perception capabilities to identify and locate items, and precise execution capabilities to manipulate and navigate. In addition, the framework’s comprehensive evaluation approach can accelerate progress in the field of medical robotics, promoting the development of assistive robots that can understand complex care instructions, perceive patient needs and environmental context, and perform sophisticated assistive tasks. The benchmark shows that current VLMs struggle to achieve excellence in all three capabilities simultaneously, highlighting a key research direction for developing more powerful embodied AI systems that are critical to these real-world applications.
              </p>
              <h2 class="subtitle is-4">Reproducibility, Licencing and Access</h2>
              <p class="has-text-justified">
                To ensure reproducibility and promote future research on embodied intelligence evaluation, we have made the full Uni-Embodied benchmark publicly available on Hugging Face. The released package contains all evaluation samples for the three core components (Planning, Perception, and Execution), complete evaluation code. This open source release enables researchers to reproduce our experimental results and conduct fair comparisons with their own methods using standardized evaluation procedures. All datasets are accompanied by clear question-answer pairs. The datasets and benchmarks are under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Interational License. Researchers can access the full benchmark suite, contribute improvements, and extend the evaluation framework to new embodied intelligence tasks, thus promoting collaborative development in this key research area.
              </p>

              

            </div>
          </div>
        </div>
      </div>

    </div>
  </div>
</section>


<!-- End Abstract -->

<!-- You can now continue with your video, carousel, BibTeX, etc. -->

 
